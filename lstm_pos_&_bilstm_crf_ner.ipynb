{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ===========LSTM for POS=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x271d04f0990>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {}\n",
    "\n",
    "for st, tar in training_data:\n",
    "    for wd in st:\n",
    "        if wd not in word_to_ix:\n",
    "            word_to_ix[wd] = len(word_to_ix)\n",
    "\n",
    "print(word_to_ix)\n",
    "\n",
    "tag_to_ix = {'DET': 0, 'NN': 1, 'V': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_size, vocab_size, target_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_size\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size)\n",
    "        self.hidden2tag = nn.Linear(hidden_size, target_size)\n",
    "        self.hidden = self.initHidden()\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_dim),\n",
    "                   torch.zeros(1, 1, self.hidden_dim))\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        embed = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(embed.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_score = F.log_softmax(tag_space, dim=1)\n",
    "        \n",
    "        return tag_score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger(HIDDEN_DIM, EMBEDDING_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8214, -1.4441, -1.1264],\n",
      "        [-0.8110, -1.4408, -1.1430],\n",
      "        [-0.7254, -1.5179, -1.2150],\n",
      "        [-0.7427, -1.5127, -1.1911],\n",
      "        [-0.8500, -1.4378, -1.0932]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    input_ = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    score = model(input_)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(300):\n",
    "    for sentence, tag in training_data:\n",
    "        model.zero_grad()\n",
    "        \n",
    "        model.hidden = model.initHidden()\n",
    "        \n",
    "        input_ = prepare_sequence(sentence, word_to_ix)\n",
    "        tag_score = model(input_)\n",
    "        target_ = prepare_sequence(tag, tag_to_ix)\n",
    "        \n",
    "        loss = loss_function(tag_score, target_)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3051, -1.4199, -3.8538],\n",
      "        [-4.4802, -0.0415, -3.5289],\n",
      "        [-3.4153, -3.3745, -0.0695],\n",
      "        [-0.0492, -3.4455, -4.1279],\n",
      "        [-4.5835, -0.0167, -5.0532]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    input_ = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    score = model(input_)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADVANCED: MAKING DYNAMIC DECISIONS AND THE BI-LSTM CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1dce5065c50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(vec):\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log_sum_exp in a stable way\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    \n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, hidden_dim, vocab_size, embedding_dim, tag_to_ix):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.tag_size = len(tag_to_ix)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        \n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True)\n",
    "        \n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tag_size)\n",
    "        \n",
    "        #transition matrix\n",
    "        self.transition = nn.Parameter(torch.randn(self.tag_size, self.tag_size))\n",
    "        \n",
    "        self.transition.data[tag_to_ix['START_TAG'], :] = -10000\n",
    "        self.transition.data[:, tag_to_ix['END_TAG']] = -10000\n",
    "        \n",
    "        self.hidden = self.initHidden()\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2), torch.randn(2, 1, self.hidden_dim // 2))\n",
    "    \n",
    "    def _forward_alg(self, feats):\n",
    "        init_alphas = torch.full((1, self.tag_size), -10000)\n",
    "        init_alphas[0][self.tag_to_ix['START_TAG']] = 0\n",
    "        \n",
    "        forward_var = init_alphas\n",
    "        \n",
    "        for feat in feats:\n",
    "            forward_t = []\n",
    "            \n",
    "            for next_tag in range(self.tag_size):\n",
    "                emit_score = feat[next_tag].view(1, -1)\n",
    "                emit_socre_broadcast = emit_score.expand(1, self.tag_size)\n",
    "                \n",
    "                trans_score = self.transition[next_tag].view(1, -1)\n",
    "                \n",
    "                next_for_var = forward_var + trans_score + emit_socre_broadcast\n",
    "                \n",
    "                forward_t.append(log_sum_exp(next_for_var).view(1))\n",
    "            \n",
    "            forward_var = torch.cat(forward_t).view(1, -1)\n",
    "        \n",
    "        terminal_var = forward_var + self.transition[self.tag_to_ix['END_TAG']]      \n",
    "        terminal_score = log_sum_exp(terminal_var)\n",
    "            \n",
    "        return terminal_score\n",
    "        \n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.initHidden()\n",
    "        word_embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        output, self.hidden = self.lstm(word_embeds, self.hidden)\n",
    "        output = output.view(len(sentence), self.hidden_dim)\n",
    "        feats = self.hidden2tag(output)\n",
    "        \n",
    "        return feats\n",
    "    \n",
    "    def _viterbi_decode(self, feats):\n",
    "        back_pointer = []\n",
    "        \n",
    "        init_vvars = torch.full((1, self.tag_size), -10000)\n",
    "        init_vvars[0][self.tag_to_ix['START_TAG']] = 0\n",
    "        \n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            cur_var = []\n",
    "            cur_back = []\n",
    "            for next_tag in range(self.tag_size):\n",
    "                trans_score = self.transition[next_tag]\n",
    "                next_tag_var = forward_var + trans_score\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                cur_back.append(best_tag_id)\n",
    "                cur_var.append(next_tag_var[0, best_tag_id].view(1))\n",
    "            \n",
    "            back_pointer.append(cur_back)\n",
    "            forward_var = (torch.cat(cur_var) + feat).view(1, -1)\n",
    "            \n",
    "        terminal_var = forward_var + self.transition[self.tag_to_ix['END_TAG']]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0, best_tag_id].view(1)\n",
    "        \n",
    "        best_path = [best_tag_id]\n",
    "        \n",
    "        for cur_back in reversed(back_pointer):\n",
    "            #print(best_tag_id)\n",
    "            best_tag_id = cur_back[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "                    \n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix['START_TAG']\n",
    "        best_path.reverse()\n",
    "        \n",
    "        return path_score, best_path\n",
    "        \n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        score = torch.zeros(1)     \n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix['START_TAG']], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + self.transition[tags[i+1], tags[i]].view(1) + feat[tags[i+1]].view(1)\n",
    "        \n",
    "        score += self.transition[self.tag_to_ix['END_TAG'], tags[-1]].view(1)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "    \n",
    "    def neg_log_likelihood2(self, sentence, tags):\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "        decode_score, decode_tags = self._viterbi_decode(lstm_feats)\n",
    "        gold_score = self._score_sentence(lstm_feats, tags)\n",
    "\n",
    "        same_len = sum([1 if i == j else 0 for i, j in zip(tags, decode_tags)])\n",
    "        #if same_len == len(tags):\n",
    "            #return torch.tensor([0], dtype=torch.long)\n",
    "        loss_p1 = max((20 - gold_score), 0.1)\n",
    "        loss_p2 = gold_score - (gold_score / len(tags) * same_len)\n",
    "        loss_p3 = decode_score - (decode_score / len(tags) * same_len)\n",
    "        loss = loss_p1 + loss_p2 + loss_p3\n",
    "        return loss\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TAG = \"START_TAG\"\n",
    "END_TAG = \"END_TAG\"\n",
    "EMBEDDING_DIM = 5\n",
    "HIDDEN_DIM = 4\n",
    "\n",
    "# Make up some training data\n",
    "training_data = [(\n",
    "    \"the wall street journal reported today that apple corporation made money\".split(),\n",
    "    \"B I I I O O O B I O O\".split()\n",
    "), (\n",
    "    \"georgia tech is a university in georgia\".split(),\n",
    "    \"B I O O O O B\".split()\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, \"START_TAG\": 3, \"END_TAG\": 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF(HIDDEN_DIM, len(word_to_ix), EMBEDDING_DIM, tag_to_ix)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 1, 2, 2, 2, 0, 1, 2, 2]\n",
      "(tensor([20.9057]), [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    precheck_sentence = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    precheck_tag_seq = [tag_to_ix[tag] for tag in training_data[0][1]]\n",
    "    print(precheck_tag_seq)\n",
    "    print(model(precheck_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use neg_log_likelihood(use the forward alg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13.8975], grad_fn=<ThSubBackward>)\n",
      "tensor([9.0473], grad_fn=<ThSubBackward>)\n",
      "tensor([13.3586], grad_fn=<ThSubBackward>)\n",
      "tensor([8.8260], grad_fn=<ThSubBackward>)\n",
      "tensor([13.0316], grad_fn=<ThSubBackward>)\n",
      "tensor([8.5113], grad_fn=<ThSubBackward>)\n",
      "tensor([12.7835], grad_fn=<ThSubBackward>)\n",
      "tensor([8.1712], grad_fn=<ThSubBackward>)\n",
      "tensor([12.3300], grad_fn=<ThSubBackward>)\n",
      "tensor([7.9499], grad_fn=<ThSubBackward>)\n",
      "tensor([12.0882], grad_fn=<ThSubBackward>)\n",
      "tensor([7.7376], grad_fn=<ThSubBackward>)\n",
      "tensor([11.6955], grad_fn=<ThSubBackward>)\n",
      "tensor([7.4244], grad_fn=<ThSubBackward>)\n",
      "tensor([11.4362], grad_fn=<ThSubBackward>)\n",
      "tensor([7.3769], grad_fn=<ThSubBackward>)\n",
      "tensor([11.2459], grad_fn=<ThSubBackward>)\n",
      "tensor([7.1496], grad_fn=<ThSubBackward>)\n",
      "tensor([11.0312], grad_fn=<ThSubBackward>)\n",
      "tensor([7.0471], grad_fn=<ThSubBackward>)\n",
      "tensor([10.8381], grad_fn=<ThSubBackward>)\n",
      "tensor([6.8703], grad_fn=<ThSubBackward>)\n",
      "tensor([10.6708], grad_fn=<ThSubBackward>)\n",
      "tensor([6.8352], grad_fn=<ThSubBackward>)\n",
      "tensor([10.5042], grad_fn=<ThSubBackward>)\n",
      "tensor([6.6772], grad_fn=<ThSubBackward>)\n",
      "tensor([10.3734], grad_fn=<ThSubBackward>)\n",
      "tensor([6.4746], grad_fn=<ThSubBackward>)\n",
      "tensor([10.2202], grad_fn=<ThSubBackward>)\n",
      "tensor([6.4227], grad_fn=<ThSubBackward>)\n",
      "tensor([10.1362], grad_fn=<ThSubBackward>)\n",
      "tensor([6.1811], grad_fn=<ThSubBackward>)\n",
      "tensor([10.0649], grad_fn=<ThSubBackward>)\n",
      "tensor([6.1849], grad_fn=<ThSubBackward>)\n",
      "tensor([10.0043], grad_fn=<ThSubBackward>)\n",
      "tensor([6.0832], grad_fn=<ThSubBackward>)\n",
      "tensor([9.8447], grad_fn=<ThSubBackward>)\n",
      "tensor([5.8913], grad_fn=<ThSubBackward>)\n",
      "tensor([9.8452], grad_fn=<ThSubBackward>)\n",
      "tensor([6.0572], grad_fn=<ThSubBackward>)\n",
      "tensor([9.6060], grad_fn=<ThSubBackward>)\n",
      "tensor([5.6733], grad_fn=<ThSubBackward>)\n",
      "tensor([9.5327], grad_fn=<ThSubBackward>)\n",
      "tensor([5.7419], grad_fn=<ThSubBackward>)\n",
      "tensor([9.4241], grad_fn=<ThSubBackward>)\n",
      "tensor([5.6096], grad_fn=<ThSubBackward>)\n",
      "tensor([9.4339], grad_fn=<ThSubBackward>)\n",
      "tensor([5.5835], grad_fn=<ThSubBackward>)\n",
      "tensor([9.2184], grad_fn=<ThSubBackward>)\n",
      "tensor([5.7569], grad_fn=<ThSubBackward>)\n",
      "tensor([9.1347], grad_fn=<ThSubBackward>)\n",
      "tensor([5.6148], grad_fn=<ThSubBackward>)\n",
      "tensor([9.1466], grad_fn=<ThSubBackward>)\n",
      "tensor([5.4629], grad_fn=<ThSubBackward>)\n",
      "tensor([8.9601], grad_fn=<ThSubBackward>)\n",
      "tensor([5.4309], grad_fn=<ThSubBackward>)\n",
      "tensor([8.9405], grad_fn=<ThSubBackward>)\n",
      "tensor([5.1989], grad_fn=<ThSubBackward>)\n",
      "tensor([8.8036], grad_fn=<ThSubBackward>)\n",
      "tensor([5.0861], grad_fn=<ThSubBackward>)\n",
      "tensor([8.8144], grad_fn=<ThSubBackward>)\n",
      "tensor([5.2019], grad_fn=<ThSubBackward>)\n",
      "tensor([8.8006], grad_fn=<ThSubBackward>)\n",
      "tensor([5.1866], grad_fn=<ThSubBackward>)\n",
      "tensor([8.5700], grad_fn=<ThSubBackward>)\n",
      "tensor([5.1308], grad_fn=<ThSubBackward>)\n",
      "tensor([8.5765], grad_fn=<ThSubBackward>)\n",
      "tensor([5.2248], grad_fn=<ThSubBackward>)\n",
      "tensor([8.3024], grad_fn=<ThSubBackward>)\n",
      "tensor([5.1545], grad_fn=<ThSubBackward>)\n",
      "tensor([8.3967], grad_fn=<ThSubBackward>)\n",
      "tensor([5.1120], grad_fn=<ThSubBackward>)\n",
      "tensor([8.3311], grad_fn=<ThSubBackward>)\n",
      "tensor([4.9131], grad_fn=<ThSubBackward>)\n",
      "tensor([8.2709], grad_fn=<ThSubBackward>)\n",
      "tensor([4.9587], grad_fn=<ThSubBackward>)\n",
      "tensor([8.1895], grad_fn=<ThSubBackward>)\n",
      "tensor([4.9914], grad_fn=<ThSubBackward>)\n",
      "tensor([7.9787], grad_fn=<ThSubBackward>)\n",
      "tensor([4.8132], grad_fn=<ThSubBackward>)\n",
      "tensor([7.9507], grad_fn=<ThSubBackward>)\n",
      "tensor([4.7930], grad_fn=<ThSubBackward>)\n",
      "tensor([8.0667], grad_fn=<ThSubBackward>)\n",
      "tensor([4.8476], grad_fn=<ThSubBackward>)\n",
      "tensor([7.8646], grad_fn=<ThSubBackward>)\n",
      "tensor([4.7100], grad_fn=<ThSubBackward>)\n",
      "tensor([7.9573], grad_fn=<ThSubBackward>)\n",
      "tensor([4.6630], grad_fn=<ThSubBackward>)\n",
      "tensor([7.9213], grad_fn=<ThSubBackward>)\n",
      "tensor([4.6939], grad_fn=<ThSubBackward>)\n",
      "tensor([7.6525], grad_fn=<ThSubBackward>)\n",
      "tensor([4.4776], grad_fn=<ThSubBackward>)\n",
      "tensor([7.5571], grad_fn=<ThSubBackward>)\n",
      "tensor([4.4901], grad_fn=<ThSubBackward>)\n",
      "tensor([7.6605], grad_fn=<ThSubBackward>)\n",
      "tensor([4.6488], grad_fn=<ThSubBackward>)\n",
      "tensor([7.5656], grad_fn=<ThSubBackward>)\n",
      "tensor([4.6093], grad_fn=<ThSubBackward>)\n",
      "tensor([7.5299], grad_fn=<ThSubBackward>)\n",
      "tensor([4.4871], grad_fn=<ThSubBackward>)\n",
      "tensor([7.4694], grad_fn=<ThSubBackward>)\n",
      "tensor([4.4471], grad_fn=<ThSubBackward>)\n",
      "tensor([7.4950], grad_fn=<ThSubBackward>)\n",
      "tensor([4.5984], grad_fn=<ThSubBackward>)\n",
      "tensor([7.3174], grad_fn=<ThSubBackward>)\n",
      "tensor([4.4793], grad_fn=<ThSubBackward>)\n",
      "tensor([7.4949], grad_fn=<ThSubBackward>)\n",
      "tensor([4.3342], grad_fn=<ThSubBackward>)\n",
      "tensor([7.2362], grad_fn=<ThSubBackward>)\n",
      "tensor([4.4594], grad_fn=<ThSubBackward>)\n",
      "tensor([7.1815], grad_fn=<ThSubBackward>)\n",
      "tensor([4.3976], grad_fn=<ThSubBackward>)\n",
      "tensor([7.0244], grad_fn=<ThSubBackward>)\n",
      "tensor([4.2086], grad_fn=<ThSubBackward>)\n",
      "tensor([7.1536], grad_fn=<ThSubBackward>)\n",
      "tensor([4.1752], grad_fn=<ThSubBackward>)\n",
      "tensor([7.3286], grad_fn=<ThSubBackward>)\n",
      "tensor([4.1461], grad_fn=<ThSubBackward>)\n",
      "tensor([7.1686], grad_fn=<ThSubBackward>)\n",
      "tensor([4.1087], grad_fn=<ThSubBackward>)\n",
      "tensor([7.0114], grad_fn=<ThSubBackward>)\n",
      "tensor([4.2719], grad_fn=<ThSubBackward>)\n",
      "tensor([6.8840], grad_fn=<ThSubBackward>)\n",
      "tensor([4.2731], grad_fn=<ThSubBackward>)\n",
      "tensor([7.0077], grad_fn=<ThSubBackward>)\n",
      "tensor([4.3209], grad_fn=<ThSubBackward>)\n",
      "tensor([6.9112], grad_fn=<ThSubBackward>)\n",
      "tensor([3.9457], grad_fn=<ThSubBackward>)\n",
      "tensor([6.7098], grad_fn=<ThSubBackward>)\n",
      "tensor([3.9957], grad_fn=<ThSubBackward>)\n",
      "tensor([6.6736], grad_fn=<ThSubBackward>)\n",
      "tensor([4.0436], grad_fn=<ThSubBackward>)\n",
      "tensor([6.7047], grad_fn=<ThSubBackward>)\n",
      "tensor([3.9508], grad_fn=<ThSubBackward>)\n",
      "tensor([6.6875], grad_fn=<ThSubBackward>)\n",
      "tensor([3.9266], grad_fn=<ThSubBackward>)\n",
      "tensor([6.5522], grad_fn=<ThSubBackward>)\n",
      "tensor([4.1124], grad_fn=<ThSubBackward>)\n",
      "tensor([6.6528], grad_fn=<ThSubBackward>)\n",
      "tensor([4.0834], grad_fn=<ThSubBackward>)\n",
      "tensor([6.6257], grad_fn=<ThSubBackward>)\n",
      "tensor([3.8187], grad_fn=<ThSubBackward>)\n",
      "tensor([6.6644], grad_fn=<ThSubBackward>)\n",
      "tensor([3.9676], grad_fn=<ThSubBackward>)\n",
      "tensor([6.7769], grad_fn=<ThSubBackward>)\n",
      "tensor([3.8363], grad_fn=<ThSubBackward>)\n",
      "tensor([6.5915], grad_fn=<ThSubBackward>)\n",
      "tensor([3.7537], grad_fn=<ThSubBackward>)\n",
      "tensor([6.6729], grad_fn=<ThSubBackward>)\n",
      "tensor([3.7047], grad_fn=<ThSubBackward>)\n",
      "tensor([6.3566], grad_fn=<ThSubBackward>)\n",
      "tensor([3.7647], grad_fn=<ThSubBackward>)\n",
      "tensor([6.4257], grad_fn=<ThSubBackward>)\n",
      "tensor([3.7194], grad_fn=<ThSubBackward>)\n",
      "tensor([6.6200], grad_fn=<ThSubBackward>)\n",
      "tensor([3.6321], grad_fn=<ThSubBackward>)\n",
      "tensor([6.2672], grad_fn=<ThSubBackward>)\n",
      "tensor([3.5566], grad_fn=<ThSubBackward>)\n",
      "tensor([6.5141], grad_fn=<ThSubBackward>)\n",
      "tensor([3.7488], grad_fn=<ThSubBackward>)\n",
      "tensor([6.1868], grad_fn=<ThSubBackward>)\n",
      "tensor([3.7252], grad_fn=<ThSubBackward>)\n",
      "tensor([6.5948], grad_fn=<ThSubBackward>)\n",
      "tensor([3.6201], grad_fn=<ThSubBackward>)\n",
      "tensor([6.1309], grad_fn=<ThSubBackward>)\n",
      "tensor([3.6475], grad_fn=<ThSubBackward>)\n",
      "tensor([6.3917], grad_fn=<ThSubBackward>)\n",
      "tensor([3.8576], grad_fn=<ThSubBackward>)\n",
      "tensor([6.3147], grad_fn=<ThSubBackward>)\n",
      "tensor([3.4886], grad_fn=<ThSubBackward>)\n",
      "tensor([6.0740], grad_fn=<ThSubBackward>)\n",
      "tensor([3.4164], grad_fn=<ThSubBackward>)\n",
      "tensor([5.9773], grad_fn=<ThSubBackward>)\n",
      "tensor([3.7672], grad_fn=<ThSubBackward>)\n",
      "tensor([6.0138], grad_fn=<ThSubBackward>)\n",
      "tensor([3.7814], grad_fn=<ThSubBackward>)\n",
      "tensor([5.8393], grad_fn=<ThSubBackward>)\n",
      "tensor([3.6650], grad_fn=<ThSubBackward>)\n",
      "tensor([5.9893], grad_fn=<ThSubBackward>)\n",
      "tensor([3.3467], grad_fn=<ThSubBackward>)\n",
      "tensor([6.1109], grad_fn=<ThSubBackward>)\n",
      "tensor([3.9106], grad_fn=<ThSubBackward>)\n",
      "tensor([5.9234], grad_fn=<ThSubBackward>)\n",
      "tensor([3.4991], grad_fn=<ThSubBackward>)\n",
      "tensor([5.8266], grad_fn=<ThSubBackward>)\n",
      "tensor([3.3664], grad_fn=<ThSubBackward>)\n",
      "tensor([5.9411], grad_fn=<ThSubBackward>)\n",
      "tensor([3.2931], grad_fn=<ThSubBackward>)\n",
      "tensor([5.7025], grad_fn=<ThSubBackward>)\n",
      "tensor([3.2242], grad_fn=<ThSubBackward>)\n",
      "tensor([5.8828], grad_fn=<ThSubBackward>)\n",
      "tensor([3.3612], grad_fn=<ThSubBackward>)\n",
      "tensor([5.6659], grad_fn=<ThSubBackward>)\n",
      "tensor([3.3966], grad_fn=<ThSubBackward>)\n",
      "tensor([5.9929], grad_fn=<ThSubBackward>)\n",
      "tensor([3.2627], grad_fn=<ThSubBackward>)\n",
      "tensor([5.8916], grad_fn=<ThSubBackward>)\n",
      "tensor([3.2022], grad_fn=<ThSubBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.7185], grad_fn=<ThSubBackward>)\n",
      "tensor([3.2895], grad_fn=<ThSubBackward>)\n",
      "tagets :  tensor([0, 1, 1, 1, 2, 2, 2, 0, 1, 2, 2])\n",
      "pre :  (tensor([18.8147]), [0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "================\n",
      "tagets :  tensor([0, 1, 2, 2, 2, 2, 0])\n",
      "pre :  (tensor([11.4065]), [0, 1, 2, 2, 2, 2, 2])\n",
      "================\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    \n",
    "    for sentence, tags in training_data:\n",
    "        model.zero_grad()\n",
    "        model.initHidden()\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for sentence, tags in training_data:\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "        \n",
    "        print('tagets : ', targets)\n",
    "        print('pre : ', model(sentence_in))\n",
    "        print('================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use neg_log_likelihood2 (cal the loss use the viterbe decode path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([27.3779], grad_fn=<ThAddBackward>)\n",
      "tensor([18.9753], grad_fn=<ThAddBackward>)\n",
      "tensor([27.1541], grad_fn=<ThAddBackward>)\n",
      "tensor([18.9429], grad_fn=<ThAddBackward>)\n",
      "tensor([26.5656], grad_fn=<ThAddBackward>)\n",
      "tensor([19.0123], grad_fn=<ThAddBackward>)\n",
      "tensor([26.2677], grad_fn=<ThAddBackward>)\n",
      "tensor([19.0848], grad_fn=<ThAddBackward>)\n",
      "tensor([25.0882], grad_fn=<ThAddBackward>)\n",
      "tensor([17.6480], grad_fn=<ThAddBackward>)\n",
      "tensor([24.8263], grad_fn=<ThAddBackward>)\n",
      "tensor([17.7330], grad_fn=<ThAddBackward>)\n",
      "tensor([24.6369], grad_fn=<ThAddBackward>)\n",
      "tensor([17.5252], grad_fn=<ThAddBackward>)\n",
      "tensor([24.5979], grad_fn=<ThAddBackward>)\n",
      "tensor([17.5009], grad_fn=<ThAddBackward>)\n",
      "tensor([24.2695], grad_fn=<ThAddBackward>)\n",
      "tensor([17.3153], grad_fn=<ThAddBackward>)\n",
      "tensor([24.2258], grad_fn=<ThAddBackward>)\n",
      "tensor([17.3741], grad_fn=<ThAddBackward>)\n",
      "tensor([24.0904], grad_fn=<ThAddBackward>)\n",
      "tensor([17.1789], grad_fn=<ThAddBackward>)\n",
      "tensor([23.8908], grad_fn=<ThAddBackward>)\n",
      "tensor([17.0328], grad_fn=<ThAddBackward>)\n",
      "tensor([23.8616], grad_fn=<ThAddBackward>)\n",
      "tensor([16.9821], grad_fn=<ThAddBackward>)\n",
      "tensor([23.4153], grad_fn=<ThAddBackward>)\n",
      "tensor([16.9771], grad_fn=<ThAddBackward>)\n",
      "tensor([23.6039], grad_fn=<ThAddBackward>)\n",
      "tensor([16.8266], grad_fn=<ThAddBackward>)\n",
      "tensor([23.4040], grad_fn=<ThAddBackward>)\n",
      "tensor([16.9023], grad_fn=<ThAddBackward>)\n",
      "tensor([23.2319], grad_fn=<ThAddBackward>)\n",
      "tensor([16.8456], grad_fn=<ThAddBackward>)\n",
      "tensor([23.1116], grad_fn=<ThAddBackward>)\n",
      "tensor([16.6074], grad_fn=<ThAddBackward>)\n",
      "tensor([22.8651], grad_fn=<ThAddBackward>)\n",
      "tensor([16.7404], grad_fn=<ThAddBackward>)\n",
      "tensor([22.8749], grad_fn=<ThAddBackward>)\n",
      "tensor([16.6409], grad_fn=<ThAddBackward>)\n",
      "tensor([22.7845], grad_fn=<ThAddBackward>)\n",
      "tensor([16.6189], grad_fn=<ThAddBackward>)\n",
      "tensor([22.5846], grad_fn=<ThAddBackward>)\n",
      "tensor([16.3500], grad_fn=<ThAddBackward>)\n",
      "tensor([22.4471], grad_fn=<ThAddBackward>)\n",
      "tensor([16.1221], grad_fn=<ThAddBackward>)\n",
      "tensor([21.1996], grad_fn=<ThAddBackward>)\n",
      "tensor([16.0359], grad_fn=<ThAddBackward>)\n",
      "tensor([22.5180], grad_fn=<ThAddBackward>)\n",
      "tensor([21.1206], grad_fn=<ThAddBackward>)\n",
      "tensor([22.4154], grad_fn=<ThAddBackward>)\n",
      "tensor([16.0974], grad_fn=<ThAddBackward>)\n",
      "tensor([20.9466], grad_fn=<ThAddBackward>)\n",
      "tensor([15.8236], grad_fn=<ThAddBackward>)\n",
      "tensor([22.3050], grad_fn=<ThAddBackward>)\n",
      "tensor([15.4956], grad_fn=<ThAddBackward>)\n",
      "tensor([20.6861], grad_fn=<ThAddBackward>)\n",
      "tensor([15.4391], grad_fn=<ThAddBackward>)\n",
      "tensor([22.2648], grad_fn=<ThAddBackward>)\n",
      "tensor([15.4271], grad_fn=<ThAddBackward>)\n",
      "tensor([20.5525], grad_fn=<ThAddBackward>)\n",
      "tensor([15.0387], grad_fn=<ThAddBackward>)\n",
      "tensor([22.0573], grad_fn=<ThAddBackward>)\n",
      "tensor([15.1374], grad_fn=<ThAddBackward>)\n",
      "tensor([20.3878], grad_fn=<ThAddBackward>)\n",
      "tensor([15.3593], grad_fn=<ThAddBackward>)\n",
      "tensor([22.3552], grad_fn=<ThAddBackward>)\n",
      "tensor([21.0727], grad_fn=<ThAddBackward>)\n",
      "tensor([22.1232], grad_fn=<ThAddBackward>)\n",
      "tensor([12.6933], grad_fn=<ThAddBackward>)\n",
      "tensor([23.6649], grad_fn=<ThAddBackward>)\n",
      "tensor([14.5678], grad_fn=<ThAddBackward>)\n",
      "tensor([21.9374], grad_fn=<ThAddBackward>)\n",
      "tensor([14.5183], grad_fn=<ThAddBackward>)\n",
      "tensor([21.9342], grad_fn=<ThAddBackward>)\n",
      "tensor([12.1264], grad_fn=<ThAddBackward>)\n",
      "tensor([21.9122], grad_fn=<ThAddBackward>)\n",
      "tensor([11.9328], grad_fn=<ThAddBackward>)\n",
      "tensor([23.9309], grad_fn=<ThAddBackward>)\n",
      "tensor([11.7544], grad_fn=<ThAddBackward>)\n",
      "tensor([23.8768], grad_fn=<ThAddBackward>)\n",
      "tensor([11.7204], grad_fn=<ThAddBackward>)\n",
      "tensor([21.9081], grad_fn=<ThAddBackward>)\n",
      "tensor([10.8512], grad_fn=<ThAddBackward>)\n",
      "tensor([23.8644], grad_fn=<ThAddBackward>)\n",
      "tensor([10.9990], grad_fn=<ThAddBackward>)\n",
      "tensor([21.8850], grad_fn=<ThAddBackward>)\n",
      "tensor([11.4069], grad_fn=<ThAddBackward>)\n",
      "tensor([24.3599], grad_fn=<ThAddBackward>)\n",
      "tensor([10.3313], grad_fn=<ThAddBackward>)\n",
      "tensor([24.3536], grad_fn=<ThAddBackward>)\n",
      "tensor([10.7940], grad_fn=<ThAddBackward>)\n",
      "tensor([21.9062], grad_fn=<ThAddBackward>)\n",
      "tensor([10.3712], grad_fn=<ThAddBackward>)\n",
      "tensor([24.6522], grad_fn=<ThAddBackward>)\n",
      "tensor([10.1828], grad_fn=<ThAddBackward>)\n",
      "tensor([22.0458], grad_fn=<ThAddBackward>)\n",
      "tensor([9.4681], grad_fn=<ThAddBackward>)\n",
      "tensor([22.2128], grad_fn=<ThAddBackward>)\n",
      "tensor([9.3630], grad_fn=<ThAddBackward>)\n",
      "tensor([14.3511], grad_fn=<ThAddBackward>)\n",
      "tensor([8.6489], grad_fn=<ThAddBackward>)\n",
      "tensor([19.6265], grad_fn=<ThAddBackward>)\n",
      "tensor([8.4515], grad_fn=<ThAddBackward>)\n",
      "tensor([19.4562], grad_fn=<ThAddBackward>)\n",
      "tensor([8.1101], grad_fn=<ThAddBackward>)\n",
      "tensor([25.4349], grad_fn=<ThAddBackward>)\n",
      "tensor([8.0281], grad_fn=<ThAddBackward>)\n",
      "tensor([19.3225], grad_fn=<ThAddBackward>)\n",
      "tensor([7.6644], grad_fn=<ThAddBackward>)\n",
      "tensor([25.3723], grad_fn=<ThAddBackward>)\n",
      "tensor([7.1786], grad_fn=<ThAddBackward>)\n",
      "tensor([19.3656], grad_fn=<ThAddBackward>)\n",
      "tensor([7.0700], grad_fn=<ThAddBackward>)\n",
      "tensor([19.0361], grad_fn=<ThAddBackward>)\n",
      "tensor([6.4768], grad_fn=<ThAddBackward>)\n",
      "tensor([19.1256], grad_fn=<ThAddBackward>)\n",
      "tensor([6.0477], grad_fn=<ThAddBackward>)\n",
      "tensor([19.0439], grad_fn=<ThAddBackward>)\n",
      "tensor([5.6459], grad_fn=<ThAddBackward>)\n",
      "tensor([19.0985], grad_fn=<ThAddBackward>)\n",
      "tensor([5.2621], grad_fn=<ThAddBackward>)\n",
      "tensor([26.5553], grad_fn=<ThAddBackward>)\n",
      "tensor([5.4783], grad_fn=<ThAddBackward>)\n",
      "tensor([11.7351], grad_fn=<ThAddBackward>)\n",
      "tensor([4.5613], grad_fn=<ThAddBackward>)\n",
      "tensor([18.6864], grad_fn=<ThAddBackward>)\n",
      "tensor([4.2422], grad_fn=<ThAddBackward>)\n",
      "tensor([19.1176], grad_fn=<ThAddBackward>)\n",
      "tensor([4.2369], grad_fn=<ThAddBackward>)\n",
      "tensor([26.8413], grad_fn=<ThAddBackward>)\n",
      "tensor([4.7359], grad_fn=<ThAddBackward>)\n",
      "tensor([11.3064], grad_fn=<ThAddBackward>)\n",
      "tensor([3.9962], grad_fn=<ThAddBackward>)\n",
      "tensor([19.5416], grad_fn=<ThAddBackward>)\n",
      "tensor([4.3321], grad_fn=<ThAddBackward>)\n",
      "tensor([15.3349], grad_fn=<ThAddBackward>)\n",
      "tensor([4.3056], grad_fn=<ThAddBackward>)\n",
      "tensor([19.2281], grad_fn=<ThAddBackward>)\n",
      "tensor([4.6376], grad_fn=<ThAddBackward>)\n",
      "tensor([26.0005], grad_fn=<ThAddBackward>)\n",
      "tensor([4.2842], grad_fn=<ThAddBackward>)\n",
      "tensor([18.7657], grad_fn=<ThAddBackward>)\n",
      "tensor([3.6467], grad_fn=<ThAddBackward>)\n",
      "tensor([27.7400], grad_fn=<ThAddBackward>)\n",
      "tensor([3.9204], grad_fn=<ThAddBackward>)\n",
      "tensor([22.4215], grad_fn=<ThAddBackward>)\n",
      "tensor([3.9924], grad_fn=<ThAddBackward>)\n",
      "tensor([19.0561], grad_fn=<ThAddBackward>)\n",
      "tensor([4.2904], grad_fn=<ThAddBackward>)\n",
      "tensor([19.0122], grad_fn=<ThAddBackward>)\n",
      "tensor([3.4480], grad_fn=<ThAddBackward>)\n",
      "tensor([23.1904], grad_fn=<ThAddBackward>)\n",
      "tensor([3.4650], grad_fn=<ThAddBackward>)\n",
      "tensor([19.8450], grad_fn=<ThAddBackward>)\n",
      "tensor([3.9887], grad_fn=<ThAddBackward>)\n",
      "tensor([26.1681], grad_fn=<ThAddBackward>)\n",
      "tensor([3.4317], grad_fn=<ThAddBackward>)\n",
      "tensor([26.5007], grad_fn=<ThAddBackward>)\n",
      "tensor([3.5487], grad_fn=<ThAddBackward>)\n",
      "tensor([26.1636], grad_fn=<ThAddBackward>)\n",
      "tensor([3.3988], grad_fn=<ThAddBackward>)\n",
      "tensor([25.8489], grad_fn=<ThAddBackward>)\n",
      "tensor([3.3330], grad_fn=<ThAddBackward>)\n",
      "tensor([33.4167], grad_fn=<ThAddBackward>)\n",
      "tensor([3.3039], grad_fn=<ThAddBackward>)\n",
      "tensor([34.2635], grad_fn=<ThAddBackward>)\n",
      "tensor([3.3363], grad_fn=<ThAddBackward>)\n",
      "tensor([26.0630], grad_fn=<ThAddBackward>)\n",
      "tensor([3.2075], grad_fn=<ThAddBackward>)\n",
      "tensor([26.2348], grad_fn=<ThAddBackward>)\n",
      "tensor([2.8881], grad_fn=<ThAddBackward>)\n",
      "tensor([22.5228], grad_fn=<ThAddBackward>)\n",
      "tensor([2.4923], grad_fn=<ThAddBackward>)\n",
      "tensor([26.8214], grad_fn=<ThAddBackward>)\n",
      "tensor([2.6898], grad_fn=<ThAddBackward>)\n",
      "tensor([26.0298], grad_fn=<ThAddBackward>)\n",
      "tensor([2.5138], grad_fn=<ThAddBackward>)\n",
      "tensor([26.1321], grad_fn=<ThAddBackward>)\n",
      "tensor([2.3185], grad_fn=<ThAddBackward>)\n",
      "tensor([26.1866], grad_fn=<ThAddBackward>)\n",
      "tensor([2.4981], grad_fn=<ThAddBackward>)\n",
      "tensor([34.4487], grad_fn=<ThAddBackward>)\n",
      "tensor([2.4323], grad_fn=<ThAddBackward>)\n",
      "tensor([29.6414], grad_fn=<ThAddBackward>)\n",
      "tensor([2.2731], grad_fn=<ThAddBackward>)\n",
      "tensor([26.3149], grad_fn=<ThAddBackward>)\n",
      "tensor([2.2196], grad_fn=<ThAddBackward>)\n",
      "tensor([25.9817], grad_fn=<ThAddBackward>)\n",
      "tensor([2.1529], grad_fn=<ThAddBackward>)\n",
      "tensor([33.7733], grad_fn=<ThAddBackward>)\n",
      "tensor([1.7288], grad_fn=<ThAddBackward>)\n",
      "tensor([26.0481], grad_fn=<ThAddBackward>)\n",
      "tensor([1.1196], grad_fn=<ThAddBackward>)\n",
      "tensor([33.6013], grad_fn=<ThAddBackward>)\n",
      "tensor([1.3674], grad_fn=<ThAddBackward>)\n",
      "tensor([26.2631], grad_fn=<ThAddBackward>)\n",
      "tensor([1.3425], grad_fn=<ThAddBackward>)\n",
      "tensor([25.9461], grad_fn=<ThAddBackward>)\n",
      "tensor([0.6976], grad_fn=<ThAddBackward>)\n",
      "tensor([29.9668], grad_fn=<ThAddBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3206], grad_fn=<ThAddBackward>)\n",
      "tensor([36.8437], grad_fn=<ThAddBackward>)\n",
      "tensor([0.9479], grad_fn=<ThAddBackward>)\n",
      "tensor([26.2802], grad_fn=<ThAddBackward>)\n",
      "tensor([0.8124], grad_fn=<ThAddBackward>)\n",
      "tensor([33.6020], grad_fn=<ThAddBackward>)\n",
      "tensor([0.8329], grad_fn=<ThAddBackward>)\n",
      "tensor([22.4055], grad_fn=<ThAddBackward>)\n",
      "tensor([0.4276], grad_fn=<ThAddBackward>)\n",
      "tensor([26.0532], grad_fn=<ThAddBackward>)\n",
      "tensor([0.3752], grad_fn=<ThAddBackward>)\n",
      "tensor([26.1521], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([36.8111], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([22.2614], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([25.7760], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1208], grad_fn=<ThAddBackward>)\n",
      "tensor([29.5267], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([15.3543], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([18.8115], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([29.0550], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1811], grad_fn=<ThAddBackward>)\n",
      "tensor([18.5841], grad_fn=<ThAddBackward>)\n",
      "tensor([0.6376], grad_fn=<ThAddBackward>)\n",
      "tensor([18.8262], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([29.7024], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([22.4646], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([18.7389], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([22.3885], grad_fn=<ThAddBackward>)\n",
      "tensor([0.2782], grad_fn=<ThAddBackward>)\n",
      "tensor([22.2444], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([25.6820], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([15.2485], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([18.7195], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([22.1165], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([25.5925], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([22.0314], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([11.7192], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([22.2954], grad_fn=<ThAddBackward>)\n",
      "tensor([0.8348], grad_fn=<ThAddBackward>)\n",
      "tensor([18.5930], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([11.3718], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([23.8084], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([25.4418], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([21.9410], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([14.9142], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([14.8695], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([29.1460], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([15.1937], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([11.1303], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([22.5575], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1347], grad_fn=<ThAddBackward>)\n",
      "tensor([14.8894], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([14.8631], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([18.3420], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([25.5053], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([18.4211], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([18.4910], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([18.3703], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([8.1015], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([18.5634], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tensor([4.7491], grad_fn=<ThAddBackward>)\n",
      "tensor([0.1000], grad_fn=<ThAddBackward>)\n",
      "tagets :  tensor([0, 1, 1, 1, 2, 2, 2, 0, 1, 2, 2])\n",
      "pre :  (tensor([19.6814]), [0, 1, 1, 1, 2, 2, 2, 0, 1, 2, 0])\n",
      "================\n",
      "tagets :  tensor([0, 1, 2, 2, 2, 2, 0])\n",
      "pre :  (tensor([20.5437]), [0, 1, 2, 2, 2, 2, 0])\n",
      "================\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(150):\n",
    "    \n",
    "    for sentence, tags in training_data:\n",
    "        model.zero_grad()\n",
    "        model.initHidden()\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "        loss = model.neg_log_likelihood2(sentence_in, targets)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for sentence, tags in training_data:\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "        \n",
    "        print('tagets : ', targets)\n",
    "        print('pre : ', model(sentence_in))\n",
    "        print('================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
