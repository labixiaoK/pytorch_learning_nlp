{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==============Translation with a Sequence to Sequence Network and Attention==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import string\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: 'SOS', 1: 'EOS'}\n",
    "        self.n_words = 2\n",
    "        \n",
    "    def addSentence(self, sentence):\n",
    "        for wd in sentence.split(' '):\n",
    "            self.addWord(wd)\n",
    "            \n",
    "    def addWord(self, wd):\n",
    "        if wd in self.word2index:\n",
    "            self.word2count[wd] += 1\n",
    "        else:\n",
    "            self.word2index[wd] = self.n_words\n",
    "            self.word2count[wd] = 1\n",
    "            self.index2word[self.n_words] = wd\n",
    "            self.n_words += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    lines = open('classify names with char rnn/data/{}-{}.txt'.format(lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n",
    "    pairs = [[normalizeString(s) for s in line.strip().split('\\t')] for line in lines]\n",
    "    \n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lange(lang1)\n",
    "        output_lang = Lange(lang2)\n",
    "    return input_lang, output_lang, pairs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#只取一部分数据集做demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(pair):\n",
    "    return len(pair[0].strip().split(' ')) < MAX_LENGTH and len(pair[1].strip().split(' ')) < MAX_LENGTH and pair[1].strip().startswith(eng_prefixes)\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========start prepar date========\n",
      "original pairs:  135842\n",
      "after filter, paris:  10853\n",
      "Lang1/(input): fra, its vocab size: 4489\n",
      "Lang2/(output): eng, its vocab size: 2925\n",
      "['je ne suis pas surprise .', 'i m not surprised .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    print('=========start prepar date========')\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print('original pairs: ', len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print('after filter, paris: ', len(pairs))\n",
    "    \n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "        \n",
    "    print('Lang1/(input): ' + input_lang.name + ', its vocab size: ' + str(input_lang.n_words))\n",
    "    print('Lang2/(output): ' + output_lang.name + ', its vocab size: ' + str(output_lang.n_words))\n",
    "    \n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embeddings = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        #self.hidden = self.initHidden()\n",
    "        \n",
    "    def forward(self, input_, hidden):\n",
    "        embed = self.embeddings(input_).view(1, 1, -1)\n",
    "        output = embed\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.embeddings = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, input_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_, hidden):\n",
    "        embed = self.embeddings(input_).view(1, 1, -1)\n",
    "        ouput = F.relu(embed)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.linear(output[0])\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, max_len=MAX_LENGTH, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.max_len = max_len\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        self.embeddings = nn.embeddins(output_size, hidden_size)\n",
    "        self.attn = nn.Linear(hidden_size * 2, max_len)\n",
    "        self.attn_combine = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input_, hidden, encoder_outs):\n",
    "        embed = self.embeddings(input_).view(1, 1, -1)\n",
    "        embed = self.dropout(embed)\n",
    "        \n",
    "        attn_wei = F.softmax(self.attn(torch.cat((embed[0], hidden[0]), 1)), dim=1)\n",
    "        context_vec = torch.bmm(attn_wei.unsqueeze(0), encoder_outs.unsqueeze(0))\n",
    "        \n",
    "        attn_combine = self.attn_combine(torch.cat((embed[0], context_vec[0]), 1)).unsqueeze(0)\n",
    "        gru_in = F.relu(attn_combine)\n",
    "        \n",
    "        output, hidden = self.gru(gru_in, hidden)\n",
    "        out = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return out, hidden, attn_wei\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "        \n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[w] for w in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    \n",
    "    return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
